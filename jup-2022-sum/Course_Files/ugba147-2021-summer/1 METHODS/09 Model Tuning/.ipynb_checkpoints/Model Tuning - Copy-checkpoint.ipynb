{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Model Tuning\n",
    "![](banner_model_tuning.jpg)\n",
    "_<p style=\"text-align: center;\"> Which model performs best? </p>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "f = \"setup.R\"; for (i in 1:10) { if (file.exists(f)) break else f = paste0(\"../\", f) }; source(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Motivation, context, history, related topics ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Consider this pedagogical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "data = data.frame(x1=c(1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 6.5, 8.0, 9.0, 9.5, 1.0, 1.5, 2.0, 3.0, 1.5, 2.0, 3.0, 6.5, 8.0, 9.0, 9.5),\n",
    "                  x2=c(1.0, 0.5, 3.0, 4.0, 5.5, 5.0, 3.5, 4.0, 7.0, 6.5, 9.0, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 6.5, 8.0, 9.0, 9.5),\n",
    "                  x3=c(5.0, 5.5, 5.0, 5.0, 5.5, 5.0, 3.5, 4.0, 7.0, 6.5, 9.0, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 6.5, 8.0, 9.0, 9.5),\n",
    "                  class=c(\"A\",\"A\",\"B\",\"B\",\"A\",\"A\",\"A\",\"B\",\"B\",\"A\",\"B\", \"A\",\"A\",\"B\",\"B\",\"A\",\"A\",\"A\",\"B\",\"B\",\"A\",\"B\"))\n",
    "\n",
    "p1 = ggplot(data) + xlim(0,10) + ylim(0,10) + geom_point(aes(x=x1, y=x2, color=class)) + theme.legend_below\n",
    "p2 = ggplot(data) + xlim(0,10) + ylim(0,10) + geom_point(aes(x=x1, y=x3, color=class)) + theme.legend_below\n",
    "p3 = ggplot(data) + xlim(0,10) + ylim(0,10) + geom_point(aes(x=x2, y=x3, color=class)) + theme.legend_below\n",
    "\n",
    "grid.arrange(p1, p2, p3, nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list()\n",
    "for (i in 1:5) { x[[i]] = i*10 }\n",
    "x # a list\n",
    "unlist(x) # a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation (Revisited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose Number of Folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "nfold = 5\n",
    "fmt(nfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data & Testing Data for Each Fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set.seed(0)\n",
    "fold = createFolds(data$class, k=nfold)\n",
    "\n",
    "data.train = list()\n",
    "data.test  = list()\n",
    "for (i in 1:nfold) { data.train[[i]] = data[setdiff(1:nrow(data), fold[[i]]),]\n",
    "                     data.test[[i]]  = data[fold[[i]],] }\n",
    "\n",
    "layout(fmt(data.train[[1]]), fmt(data.train[[2]]), fmt(data.train[[3]]), fmt(data.train[[4]]), fmt(data.train[[5]]))\n",
    "layout(fmt(data.test[[1]]),  fmt(data.test[[2]]),  fmt(data.test[[3]]),  fmt(data.test[[4]]),  fmt(data.test[[5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Confusion Matrix for Each Fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = list()\n",
    "for (i in 1:nfold) { set.seed(0)\n",
    "                     model = svm(class ~ x1+x2+x3, data.train[[i]], kernel=\"polynomial\", degree=3, cost=10, probability=TRUE)\n",
    "                     prob = attr(predict(model, data.test[[i]], probability=TRUE), \"probabilities\")\n",
    "                     class.predicted = as.class(prob, \"A\", cutoff=0.5)\n",
    "                     CM = confusionMatrix(class.predicted, data.test[[i]]$class)$table\n",
    "                     cm[[i]] = CM/sum(CM) }\n",
    "\n",
    "layout(fmt.cm(cm[[1]]), fmt.cm(cm[[2]]), fmt.cm(cm[[3]]), fmt.cm(cm[[4]]), fmt.cm(cm[[5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Performance Measurement for Each Fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = list()\n",
    "for (i in 1:nfold) { accuracy[[i]] = cm[[i]][1,1]+cm[[i]][2,2] }\n",
    "\n",
    "layout(fmt(accuracy[[1]]), fmt(accuracy[[2]]), fmt(accuracy[[3]]), fmt(accuracy[[4]]), fmt(accuracy[[5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Performance Measurement for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracy = mean(unlist(accuracy))\n",
    "fmt(cv_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 5\n",
    "\n",
    "# Training Data & Testing Data for Each Fold \n",
    "set.seed(0)\n",
    "fold = createFolds(data$class, k=nfold)\n",
    "data.train = list()\n",
    "data.test  = list()\n",
    "for (i in 1:nfold) { data.train[[i]] = data[setdiff(1:nrow(data), fold[[i]]),]\n",
    "                     data.test[[i]]  = data[fold[[i]],] }\n",
    "\n",
    "# A Confusion Matrix for Each Fold\n",
    "cm = list()\n",
    "for (i in 1:nfold) { set.seed(0)\n",
    "                     model = svm(class ~ x1+x2, data.train[[i]], kernel=\"polynomial\", degree=3, cost=10, probability=TRUE)\n",
    "                     prob = attr(predict(model, data.test[[i]], probability=TRUE), \"probabilities\")\n",
    "                     class.predicted = as.class(prob, \"A\", cutoff=0.5)\n",
    "                     CM = confusionMatrix(class.predicted, data.test[[i]]$class)$table\n",
    "                     cm[[i]] = CM/sum(CM) }\n",
    "\n",
    "# A Performance Measurement for Each Fold \n",
    "accuracy = list()\n",
    "for (i in 1:nfold) { accuracy[[i]] = cm[[i]][1,1]+cm[[i]][2,2] }\n",
    "\n",
    "# A Performance Measurement for the Model\n",
    "cv_accuracy = mean(unlist(accuracy))\n",
    "fmt(cv_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning by Cut-off Value Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = data.frame()\n",
    "for (q in c(0.10, 0.25, 0.50, 0.75, 0.90))\n",
    "{\n",
    "   tune = rbind(tune, data.frame(cutoff=q)) \n",
    "}\n",
    "\n",
    "tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = data.frame()\n",
    "for (q in c(0.10, 0.25, 0.50, 0.75, 0.90))  # try several values for cut-off\n",
    "{ \n",
    "\n",
    "    nfold = 5\n",
    "\n",
    "    # Training Data & Testing Data for Each Fold \n",
    "    set.seed(0)\n",
    "    fold = createFolds(data$class, k=nfold)\n",
    "    data.train = list()\n",
    "    data.test  = list()\n",
    "    for (i in 1:nfold) { data.train[[i]] = data[setdiff(1:nrow(data), fold[[i]]),]\n",
    "                         data.test[[i]]  = data[fold[[i]],] }\n",
    "\n",
    "    # A Confusion Matrix for Each Fold\n",
    "    cm = list()\n",
    "    for (i in 1:nfold) { set.seed(0)\n",
    "                         model = svm(class ~ x1+x2+x3, data.train[[i]], kernel=\"polynomial\", degree=3, cost=10, probability=TRUE)\n",
    "                         prob = attr(predict(model, data.test[[i]], probability=TRUE), \"probabilities\")\n",
    "                         class.predicted = as.class(prob, \"A\", cutoff=q)\n",
    "                         CM = confusionMatrix(class.predicted, data.test[[i]]$class)$table\n",
    "                         cm[[i]] = CM/sum(CM) }\n",
    "\n",
    "    # A Performance Measurement for Each Fold \n",
    "    accuracy = list()\n",
    "    for (i in 1:nfold) { accuracy[[i]] = cm[[i]][1,1]+cm[[i]][2,2] }\n",
    "\n",
    "    # A Performance Measurement for the Model\n",
    "    cv_accuracy = mean(unlist(accuracy))\n",
    "     \n",
    "    # Gather Results\n",
    "    tune = rbind(tune, data.frame(method=\"svm\", cutoff=q, nfold, cv_accuracy))  \n",
    "    \n",
    "}\n",
    "    \n",
    "tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune[which.max(tune$cv_accuracy), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning by Hyper-Parameter Value Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tune = data.frame()\n",
    "for (j in 1:4)  # try several values for degree\n",
    "for (k in c(0.1, 1, 10, 100, 1000))  # try several values for cost   \n",
    "{\n",
    "    tune = rbind(tune, data.frame(degree=j, cost=k))   \n",
    "}\n",
    "    \n",
    "tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tune = data.frame()\n",
    "for (j in 1:4)  # try several values for degree hyper-parameter\n",
    "for (k in c(0.1, 1, 10, 100, 1000))  # try several values for cost hyper-parameter\n",
    "{ \n",
    "\n",
    "    nfold = 5\n",
    "\n",
    "    # Training Data & Testing Data for Each Fold \n",
    "    set.seed(0)\n",
    "    fold = createFolds(data$class, k=nfold)\n",
    "    data.train = list()\n",
    "    data.test  = list()\n",
    "    for (i in 1:nfold) { data.train[[i]] = data[setdiff(1:nrow(data), fold[[i]]),]\n",
    "                         data.test[[i]]  = data[fold[[i]],] }\n",
    "\n",
    "    # A Confusion Matrix for Each Fold\n",
    "    cm = list()\n",
    "    for (i in 1:nfold) { set.seed(0)\n",
    "                         model = svm(class ~ x1+x2+x3, data.train[[i]], kernel=\"polynomial\", degree=j, cost=k, probability=TRUE)\n",
    "                         prob = attr(predict(model, data.test[[i]], probability=TRUE), \"probabilities\")\n",
    "                         class.predicted = as.class(prob, \"A\", cutoff=0.5)\n",
    "                         CM = confusionMatrix(class.predicted, data.test[[i]]$class)$table\n",
    "                         cm[[i]] = CM/sum(CM) }\n",
    "\n",
    "    # A Performance Measurement for Each Fold \n",
    "    accuracy = list()\n",
    "    for (i in 1:nfold) { accuracy[[i]] = cm[[i]][1,1]+cm[[i]][2,2] }\n",
    "\n",
    "    # A Performance Measurement for the Model\n",
    "    cv_accuracy = mean(unlist(accuracy))\n",
    "     \n",
    "    # Gather Results\n",
    "    tune = rbind(tune, data.frame(method=\"svm\", degree=j, cost=k, nfold, cv_accuracy))  \n",
    "    \n",
    "}\n",
    "    \n",
    "tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune[which.max(tune$cv_accuracy), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning by Exhaustive Variable Selection\n",
    "\n",
    "Also called \"feature selection\" and \"attribute selection\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exhaustive(names(data), keep=\"class\") # a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = data.frame()\n",
    "for (f in exhaustive(names(data), keep=\"class\"))\n",
    "{\n",
    "    tune = rbind(tune, data.frame(variables=paste(f, collapse=\", \")))\n",
    "}\n",
    "\n",
    "tune # a data.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = data.frame()\n",
    "for (f in exhaustive(names(data), keep=\"class\")) # try several combinations of variables\n",
    "{ \n",
    "\n",
    "    nfold = 5\n",
    "\n",
    "    # Training Data & Testing Data for Each Fold \n",
    "    set.seed(0)\n",
    "    fold = createFolds(data$class, k=nfold)\n",
    "    data.train = list()\n",
    "    data.test  = list()\n",
    "    for (i in 1:nfold) { data.train[[i]] = data[setdiff(1:nrow(data), fold[[i]]),]\n",
    "                         data.test[[i]]  = data[fold[[i]],] }\n",
    "\n",
    "    # A Confusion Matrix for Each Fold\n",
    "    cm = list()\n",
    "    for (i in 1:nfold) { set.seed(0)\n",
    "                         model = svm(class ~ ., data.train[[i]][,f], kernel=\"polynomial\", degree=3, cost=10, probability=TRUE)\n",
    "                         prob = attr(predict(model, data.test[[i]], probability=TRUE), \"probabilities\")\n",
    "                         class.predicted = as.class(prob, \"A\", cutoff=0.5)\n",
    "                         CM = confusionMatrix(class.predicted, data.test[[i]]$class)$table\n",
    "                         cm[[i]] = CM/sum(CM) }\n",
    "\n",
    "    # A Performance Measurement for Each Fold \n",
    "    accuracy = list()\n",
    "    for (i in 1:nfold) { accuracy[[i]] = cm[[i]][1,1]+cm[[i]][2,2] }\n",
    "\n",
    "    # A Performance Measurement for the Model\n",
    "    cv_accuracy = mean(unlist(accuracy))\n",
    "     \n",
    "    # Gather Results\n",
    "    tune = rbind(tune, data.frame(method=\"svm\", variables=paste(f, collapse=\", \"), nfold, cv_accuracy))  \n",
    "    \n",
    "}\n",
    "    \n",
    "tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune[which.max(tune$cv_accuracy), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tune = data.frame()\n",
    "for (q in c(0.10, 0.25, 0.50, 0.75, 0.90))  # try several values for cut-off\n",
    "for (f in exhaustive(names(data), keep=\"class\")) # try several combinations of variables\n",
    "for (j in 1:4)  # try several values for degree\n",
    "for (k in c(0.1, 1, 10, 100, 1000))  # try several values for cost\n",
    "{ \n",
    "\n",
    "    nfold = 5\n",
    "\n",
    "    # Training Data & Testing Data for Each Fold \n",
    "    set.seed(12345)\n",
    "    fold = createFolds(data$class, k=nfold)\n",
    "    data.train = list()\n",
    "    data.test  = list()\n",
    "    for (i in 1:nfold) { data.train[[i]] = data[setdiff(1:nrow(data), fold[[i]]),]\n",
    "                         data.test[[i]]  = data[fold[[i]],] }\n",
    "\n",
    "    # A Confusion Matrix for Each Fold\n",
    "    cm = list()\n",
    "    for (i in 1:nfold) { set.seed(0)\n",
    "                         model = svm(class ~ ., data.train[[i]][,f], kernel=\"polynomial\", degree=j, cost=k, probability=TRUE)\n",
    "                         prob = attr(predict(model, data.test[[i]], probability=TRUE), \"probabilities\")\n",
    "                         class.predicted = as.class(prob, \"A\", cutoff=q)\n",
    "                         CM = confusionMatrix(class.predicted, data.test[[i]]$class)$table\n",
    "                         cm[[i]] = CM/sum(CM) }\n",
    "\n",
    "    # A Performance Measurement for Each Fold \n",
    "    accuracy = list()\n",
    "    for (i in 1:nfold) { accuracy[[i]] = cm[[i]][1,1]+cm[[i]][2,2] }\n",
    "\n",
    "    # A Performance Measurement for the Model\n",
    "    cv_accuracy = mean(unlist(accuracy))\n",
    "     \n",
    "    # Gather Results\n",
    "    tune = rbind(tune, data.frame(method=\"svm\", cutoff=q, degree=j, cost=k, variables=paste(f, collapse=\", \"), nfold, cv_accuracy))  \n",
    "    \n",
    "}\n",
    "    \n",
    "size(tune)\n",
    "tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune[which.max(tune$cv_accuracy), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as.class              # from setup.R\n",
    "# exhaustive            # from setup.R\n",
    "\n",
    "# help(confusionMatrix) # from caret library\n",
    "# help(createFolds)     # from caret library\n",
    "# help(list)            # from base library\n",
    "# help(paste)           # from base library\n",
    "# help(rbind)           # from base library\n",
    "# help(setdiff)         # from base library\n",
    "# help(unlist)          # from base library\n",
    "# help(which.max)       # from base library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectations\n",
    "\n",
    "Know about this:\n",
    "* How to evaluate model performance by cross validation, conceptually and using R.\n",
    "* How to tune a model by hyper-parameter value selection, conceptually and using R.\n",
    "* How to tune a model by variable selection, conceptually and using R.\n",
    "* How to tune a model by hyper-parameter value and variable selection, conceptually and using R.\n",
    "* Compute time effects of tuning, conceptually in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "* http://www-stat.wharton.upenn.edu/~stine/mich/DM_03.pdf\n",
    "* http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/\n",
    "* http://www.cs.cmu.edu/~./awm/tutorials/overfit10.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:left; font-size:10px;\">\n",
    "Copyright (c) Berkeley Data Analytics Group, LLC\n",
    "<span style=\"float:right;\">\n",
    "Document revised April 5, 2020\n",
    "</span>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
